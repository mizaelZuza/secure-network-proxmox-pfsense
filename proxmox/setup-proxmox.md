# Guia de Setup do Proxmox para Infraestrutura de Laborat√≥rio

Este guia detalha os passos necess√°rios para configurar o Proxmox VE como a base para hospedar as VMs do pfSense, Kubernetes e TrueNAS.

## √çndice

- [1. Configura√ß√£o das Redes Virtuais (Bridges)](#1-configura√ß√£o-das-redes-virtuais-bridges)
- [2. Upload das Imagens ISO](#2-upload-das-imagens-iso)
- [3. Provisionamento das M√°quinas Virtuais](#3-provisionamento-das-m√°quinas-virtuais-vms)
- [4. Checklist P√≥s-Cria√ß√£o](#4-checklist-p√≥s-cria√ß√£o-de-vm)
- [5. Boas Pr√°ticas e Dicas](#5-boas-pr√°ticas-e-dicas)
- [6. Troubleshooting](#6-troubleshooting)
- [7. Pr√≥ximos Passos](#7-pr√≥ximos-passos)

---

## 1. Configura√ß√£o das Redes Virtuais (Bridges)

Para garantir o isolamento e a organiza√ß√£o do tr√°fego de rede, criaremos bridges Linux distintas para cada segmento de rede.

### 1.1. Backup da Configura√ß√£o Atual

‚ö†Ô∏è **IMPORTANTE**: Sempre fa√ßa backup antes de editar arquivos de rede!

```bash
ssh root@<IP_DO_SEU_PROXMOX>
cp /etc/network/interfaces /etc/network/interfaces.backup.$(date +%Y%m%d-%H%M%S)
```

### 1.2. Editar Configura√ß√£o de Rede

```bash
nano /etc/network/interfaces
```

Adicione o seguinte conte√∫do ao final do arquivo. Este exemplo assume que `vmbr0` j√° existe e est√° conectada √† sua rede f√≠sica (WAN para o pfSense).

```ini
# /etc/network/interfaces

# ... (configura√ß√£o existente de vmbr0)

# Bridge para a rede de Servidores (ex: TrueNAS, etc.)
auto vmbr1
iface vmbr1 inet manual
    bridge-ports none
    bridge-stp off
    bridge-fd 0
    comment "LAN_SERVIDORES - Rede para Servidores e Storage"

# Bridge para a rede de Clientes/Desktops
auto vmbr2
iface vmbr2 inet manual
    bridge-ports none
    bridge-stp off
    bridge-fd 0
    comment "LAN_CLIENTES - Rede para m√°quinas de usu√°rios finais"

# Bridge para o Cluster Kubernetes
auto vmbr3
iface vmbr3 inet manual
    bridge-ports none
    bridge-stp off
    bridge-fd 0
    comment "LAN_K8S - Rede isolada para o cluster Kubernetes"
```

### 1.3. Entendendo a Configura√ß√£o das Bridges

```ini
auto vmbr1
iface vmbr1 inet manual        # ‚Üê Sem IP no Proxmox (bridge pura)
    bridge-ports none          # ‚Üê Bridge pura (sem interface f√≠sica associada)
    bridge-stp off             # ‚Üê Spanning Tree Protocol desabilitado
    bridge-fd 0                # ‚Üê Forward delay = 0 (sem atraso)
    comment "LAN_SERVIDORES"
```

**Por que `inet manual`?**
- As bridges n√£o precisam de IP no host Proxmox
- O pfSense ser√° o gateway e servidor DHCP de cada rede
- Proxmox apenas conecta as VMs entre si atrav√©s das bridges

**Resumo das redes planejadas:**

| Bridge | Rede Futura | Gateway (pfSense) | Uso |
|--------|-------------|-------------------|-----|
| vmbr0 | WAN | ISP | Internet |
| vmbr1 | 192.168.1.0/24 | 192.168.1.1 | LAN_SERVIDORES (TrueNAS) |
| vmbr2 | 192.168.2.0/24 | 192.168.2.1 | LAN_CLIENTES (Desktops) |
| vmbr3 | 192.168.3.0/24 | 192.168.3.1 | LAN_K8S (Cluster) |

> **Nota**: Os IPs ser√£o configurados no pfSense, n√£o no Proxmox.

### 1.4. Aplicar as Configura√ß√µes de Rede

**M√©todo 1: Usando `ifreload` (Recomendado)**

```bash
# Aplicar mudan√ßas sem derrubar conex√£o SSH
ifreload -a

# Verificar se as bridges foram criadas
ip link show | grep vmbr
```

**M√©todo 2: Subir interfaces individualmente**

Se `ifreload` n√£o estiver dispon√≠vel:

```bash
ifup vmbr1
ifup vmbr2
ifup vmbr3

# Verificar status
ip link show vmbr1
ip link show vmbr2
ip link show vmbr3
```

**M√©todo 3: Via Interface Web (Mais Seguro)**

1. Acesse: **Datacenter ‚Üí Node ‚Üí System ‚Üí Network**
2. Clique em **Create ‚Üí Linux Bridge**
3. Configure cada bridge:
   - **Bridge ID**: vmbr1, vmbr2, vmbr3
   - **IPv4/CIDR**: Deixe em branco
   - **Bridge ports**: Deixe vazio
   - **Comment**: Adicione descri√ß√£o
4. Clique em **Apply Configuration**

**M√©todo 4: Reiniciar servi√ßo de rede (√öltima op√ß√£o)**

‚ö†Ô∏è **ATEN√á√ÉO**: Pode derrubar sua conex√£o SSH!

```bash
systemctl restart networking
```

Se perder SSH, acesse pelo console web do Proxmox: **Datacenter ‚Üí Node ‚Üí >_Shell**

### 1.5. Verificar Configura√ß√£o

```bash
# 1. Listar todas as bridges
ip link show type bridge

# 2. Ver detalhes das bridges com brctl
brctl show

# 3. Verificar estado (deve mostrar UP)
ip link show vmbr1 | grep "state UP"
ip link show vmbr2 | grep "state UP"
ip link show vmbr3 | grep "state UP"

# 4. Ver configura√ß√£o completa
cat /etc/network/interfaces | grep -A 5 "vmbr"

# 5. Verificar na interface web
# Datacenter ‚Üí Node ‚Üí System ‚Üí Network
# Todas as bridges devem aparecer com status "Active"
```

**Resultado esperado:**

```
vmbr0: <BROADCAST,MULTICAST,UP,LOWER_UP>
vmbr1: <BROADCAST,MULTICAST,UP,LOWER_UP>
vmbr2: <BROADCAST,MULTICAST,UP,LOWER_UP>
vmbr3: <BROADCAST,MULTICAST,UP,LOWER_UP>
```

---

## 2. Upload das Imagens ISO

Antes de criar as VMs, fa√ßa o upload das imagens ISO necess√°rias para o storage local do Proxmox.

### 2.1. Via Interface Web (Recomendado)

1. Acesse a interface web do Proxmox
2. Navegue at√© **Datacenter ‚Üí (seu n√≥) ‚Üí local (ou storage de sua escolha) ‚Üí ISO Images**
3. Clique em **Upload** e selecione os seguintes arquivos ISO do seu computador:
   - **pfSense**: Imagem de instala√ß√£o do pfSense (ex: `pfSense-CE-2.7.2-RELEASE-amd64.iso`)
   - **Ubuntu Server**: Imagem para os n√≥s do Kubernetes (ex: `ubuntu-24.04-live-server-amd64.iso`)
   - **TrueNAS SCALE**: Imagem de instala√ß√£o do TrueNAS (ex: `TrueNAS-SCALE-24.10.iso`)

### 2.2. Verificar ISOs Dispon√≠veis

```bash
# Via CLI
pvesm list local --content iso

# Ou na interface web
# Datacenter ‚Üí Node ‚Üí local ‚Üí ISO Images
```

---

## 3. Provisionamento das M√°quinas Virtuais (VMs)

Com as redes e as ISOs prontas, o pr√≥ximo passo √© criar as m√°quinas virtuais. A cria√ß√£o √© feita atrav√©s do assistente **"Create VM"** na interface web do Proxmox.

### 3.1. Recomenda√ß√µes Gerais para Cria√ß√£o de VMs

| Configura√ß√£o | Recomenda√ß√£o | Observa√ß√£o |
|--------------|--------------|------------|
| **Guest OS** | Linux 6.x kernel ou Other | Selecione conforme o SO |
| **Machine Type** | q35 (moderno) ou i440fx (legado) | q35 para Ubuntu/TrueNAS |
| **BIOS** | OVMF (UEFI) ou SeaBIOS | UEFI para sistemas modernos |
| **SCSI Controller** | VirtIO SCSI single | Melhor performance |
| **QEMU Guest Agent** | ‚úÖ Sempre habilitar | Exceto pfSense (n√£o suporta) |
| **Disk Bus/Device** | VirtIO Block | Melhor performance |
| **Disk Discard** | ‚úÖ Habilitar se SSD | Melhora gest√£o de espa√ßo |
| **CPU Type** | host | Melhor performance |
| **CPU Flags** | Deixe padr√£o | Ou adicione flags espec√≠ficas |
| **Memory Ballooning** | ‚ùå Desabilitar para VMs cr√≠ticas | Garante mem√≥ria reservada |
| **Network Model** | VirtIO (paravirtualized) | Melhor performance de rede |

### 3.2. Templates de Configura√ß√£o por Tipo de VM

#### üî• pfSense (Firewall/Router)

```yaml
General:
  VM ID: 100
  Name: pfsense
  Resource Pool: (opcional)

OS:
  ISO Image: pfSense-CE-2.7.2-RELEASE-amd64.iso
  Type: Other
  
System:
  Graphic card: Default
  Machine: Default (i440fx)
  BIOS: Default (SeaBIOS)
  SCSI Controller: VirtIO SCSI single
  Qemu Agent: ‚ùå Disabled (pfSense n√£o suporta nativamente)

Disks:
  scsi0: 32 GB
    Bus/Device: VirtIO Block
    Storage: local-lvm (ou seu storage preferido)
    Cache: Write back (ou Default)
    Discard: ‚úÖ Se SSD
    IO thread: ‚úÖ Enabled

CPU:
  Sockets: 1
  Cores: 2
  Type: host
  
Memory:
  Memory (MiB): 2048 (2GB)
  Minimum memory: 2048
  Ballooning Device: ‚ùå Disabled

Network:
  net0 (WAN):
    Bridge: vmbr0
    Model: VirtIO (paravirtualized)
    Firewall: ‚ùå Disabled (pfSense gerencia)
  net1 (LAN_SERVIDORES):
    Bridge: vmbr1
    Model: VirtIO
  net2 (LAN_CLIENTES):
    Bridge: vmbr2
    Model: VirtIO
  net3 (LAN_K8S):
    Bridge: vmbr3
    Model: VirtIO

Confirm:
  ‚úÖ Start after created (opcional)
```

**Comandos via CLI (alternativa):**

```bash
qm create 100 --name pfsense --memory 2048 --balloon 0 --cores 2 --cpu host \
  --net0 virtio,bridge=vmbr0 \
  --net1 virtio,bridge=vmbr1 \
  --net2 virtio,bridge=vmbr2 \
  --net3 virtio,bridge=vmbr3 \
  --scsi0 local-lvm:32 --scsihw virtio-scsi-single \
  --ide2 local:iso/pfSense-CE-2.7.2-RELEASE-amd64.iso,media=cdrom \
  --boot order=scsi0 --ostype other
```

---

#### üíæ TrueNAS SCALE (Storage NFS)

```yaml
General:
  VM ID: 200
  Name: truenas
  
OS:
  ISO Image: TrueNAS-SCALE-24.10.0.iso
  Type: Linux 6.x - 2.6 Kernel

System:
  Graphic card: Default
  Machine: q35
  BIOS: OVMF (UEFI)
  Add EFI Disk: ‚úÖ Yes
  EFI Storage: local-lvm
  SCSI Controller: VirtIO SCSI single
  Qemu Agent: ‚úÖ Enabled

Disks:
  scsi0 (Boot): 32 GB
    Storage: local-lvm
    Bus/Device: VirtIO Block
    Cache: Write back
    Discard: ‚úÖ Se SSD
    IO thread: ‚úÖ Enabled
    
  scsi1 (Data Pool 1): 100 GB
    Storage: local-lvm
    Bus/Device: VirtIO Block
    Discard: ‚úÖ Se SSD
    
  scsi2 (Data Pool 2): 100 GB
    Storage: local-lvm
    Bus/Device: VirtIO Block
    Discard: ‚úÖ Se SSD
  
    scsi2 (Data Pool 3): 100 GB
    Storage: local-lvm
    Bus/Device: VirtIO Block
    Discard: ‚úÖ Se SSD
  # Adicione mais discos conforme necess√°rio para seus pools ZFS

CPU:
  Sockets: 1
  Cores: 4
  Type: host

Memory:
  Memory (MiB): 8192 (8GB m√≠nimo, 16GB recomendado para ZFS)
  Minimum memory: 8192
  Ballooning Device: ‚ùå Disabled

Network:
  net0:
    Bridge: vmbr1 (LAN_SERVIDORES)
    Model: VirtIO (paravirtualized)
    Firewall: ‚ùå Disabled
```

**Comandos via CLI:**

```bash
qm create 200 --name truenas --memory 8192 --balloon 0 --cores 4 --cpu host \
  --net0 virtio,bridge=vmbr1 \
  --scsi0 local-lvm:32 --scsi1 local-lvm:100 --scsi2 local-lvm:100 --scsi3 local-lvm:100\
  --scsihw virtio-scsi-single --machine q35 --bios ovmf \
  --efidisk0 local-lvm:1 --agent enabled=1 \
  --ide2 local:iso/TrueNAS-SCALE-24.10.0.iso,media=cdrom \
  --boot order=scsi0 --ostype l26
```

---

#### üîÑ HAProxy Load Balancers (2 VMs)

```yaml
General:
  VM IDs: 291, 292
  Names: k8s-lb-01, k8s-lb-02

OS:
  ISO Image: ubuntu-24.04-live-server-amd64.iso
  Type: Linux 6.x - 2.6 Kernel

System:
  Machine: q35
  BIOS: OVMF (UEFI)
  Add EFI Disk: ‚úÖ Yes
  SCSI Controller: VirtIO SCSI single
  Qemu Agent: ‚úÖ Enabled

Disks:
  scsi0: 20 GB
    Bus/Device: VirtIO Block
    Discard: ‚úÖ Se SSD

CPU:
  Sockets: 1
  Cores: 2
  Type: host

Memory:
  Memory (MiB): 2048 (2GB)
  Ballooning Device: ‚ùå Disabled

Network:
  net0:
    Bridge: vmbr3 (LAN_K8S)
    Model: VirtIO
```

**Criar ambos LBs em sequ√™ncia:**

```bash
# LB-01
qm create 291 --name k8s-lb-01 --memory 2048 --balloon 0 --cores 2 --cpu host \
  --net0 virtio,bridge=vmbr3 --scsi0 local-lvm:20 \
  --scsihw virtio-scsi-single --machine q35 --bios ovmf \
  --efidisk0 local-lvm:1 --agent enabled=1 \
  --ide2 local:iso/ubuntu-24.04-live-server-amd64.iso,media=cdrom \
  --boot order=scsi0 --ostype l26

# LB-02
qm create 292 --name k8s-lb-02 --memory 2048 --balloon 0 --cores 2 --cpu host \
  --net0 virtio,bridge=vmbr3 --scsi0 local-lvm:20 \
  --scsihw virtio-scsi-single --machine q35 --bios ovmf \
  --efidisk0 local-lvm:1 --agent enabled=1 \
  --ide2 local:iso/ubuntu-24.04-live-server-amd64.iso,media=cdrom \
  --boot order=scsi0 --ostype l26
```

---

#### ‚ò∏Ô∏è Kubernetes Control Planes (3 VMs)

```yaml
General:
  VM IDs: 301, 302, 303
  Names: k8s-control-01, k8s-control-02, k8s-control-03

OS:
  ISO Image: ubuntu-24.04-live-server-amd64.iso
  Type: Linux 6.x - 2.6 Kernel

System:
  Machine: q35
  BIOS: OVMF (UEFI)
  Add EFI Disk: ‚úÖ Yes
  SCSI Controller: VirtIO SCSI single
  Qemu Agent: ‚úÖ Enabled

Disks:
  scsi0: 50 GB
    Bus/Device: VirtIO Block
    Discard: ‚úÖ Se SSD
    IO thread: ‚úÖ Enabled

CPU:
  Sockets: 1
  Cores: 2 (m√≠nimo, 4 recomendado)
  Type: host

Memory:
  Memory (MiB): 4096 (4GB m√≠nimo, 8GB recomendado)
  Ballooning Device: ‚ùå Disabled

Network:
  net0:
    Bridge: vmbr3 (LAN_K8S)
    Model: VirtIO
```

**Criar os 3 Control Planes:**

```bash
for i in {1..3}; do
  qm create 30$i --name k8s-control-0$i --memory 4096 --balloon 0 --cores 2 --cpu host \
    --net0 virtio,bridge=vmbr3 --scsi0 local-lvm:50 \
    --scsihw virtio-scsi-single --machine q35 --bios ovmf \
    --efidisk0 local-lvm:1 --agent enabled=1 \
    --ide2 local:iso/ubuntu-24.04-live-server-amd64.iso,media=cdrom \
    --boot order=scsi0 --ostype l26
done
```

---

#### üîß Kubernetes Workers (2 VMs)

```yaml
General:
  VM IDs: 311, 312
  Names: k8s-worker-01, k8s-worker-02

OS:
  ISO Image: ubuntu-24.04-live-server-amd64.iso
  Type: Linux 6.x - 2.6 Kernel

System:
  Machine: q35
  BIOS: OVMF (UEFI)
  Add EFI Disk: ‚úÖ Yes
  SCSI Controller: VirtIO SCSI single
  Qemu Agent: ‚úÖ Enabled

Disks:
  scsi0: 100 GB (ou mais, dependendo das cargas de trabalho)
    Bus/Device: VirtIO Block
    Discard: ‚úÖ Se SSD
    IO thread: ‚úÖ Enabled

CPU:
  Sockets: 1
  Cores: 4 (m√≠nimo, 8 recomendado para produ√ß√£o)
  Type: host

Memory:
  Memory (MiB): 8192 (8GB m√≠nimo, 16GB+ recomendado)
  Ballooning Device: ‚ùå Disabled

Network:
  net0:
    Bridge: vmbr3 (LAN_K8S)
    Model: VirtIO
```

**Criar os 2 Workers:**

```bash
for i in {1..2}; do
  qm create 31$i --name k8s-worker-0$i --memory 8192 --balloon 0 --cores 4 --cpu host \
    --net0 virtio,bridge=vmbr3 --scsi0 local-lvm:100 \
    --scsihw virtio-scsi-single --machine q35 --bios ovmf \
    --efidisk0 local-lvm:1 --agent enabled=1 \
    --ide2 local:iso/ubuntu-24.04-live-server-amd64.iso,media=cdrom \
    --boot order=scsi0 --ostype l26
done
```

---

### 3.3. Resumo das VMs Criadas

| VM ID | Nome | Tipo | CPU | RAM | Disco | Bridge |
|-------|------|------|-----|-----|-------|--------|
| 100 | pfsense | Firewall | 2 | 2GB | 32GB | vmbr0-3 |
| 200 | truenas | Storage | 4 | 8GB | 32+100+100GB+100GB | vmbr1 |
| 291 | k8s-lb-01 | Load Balancer | 2 | 2GB | 20GB | vmbr3 |
| 292 | k8s-lb-02 | Load Balancer | 2 | 2GB | 20GB | vmbr3 |
| 301 | k8s-control-01 | Control Plane | 2 | 4GB | 50GB | vmbr3 |
| 302 | k8s-control-02 | Control Plane | 2 | 4GB | 50GB | vmbr3 |
| 303 | k8s-control-03 | Control Plane | 2 | 4GB | 50GB | vmbr3 |
| 311 | k8s-worker-01 | Worker | 4 | 8GB | 100GB | vmbr3 |
| 312 | k8s-worker-02 | Worker | 4 | 8GB | 100GB | vmbr3 |

**Total de recursos necess√°rios:**
- **CPUs**: 24 cores
- **RAM**: 40 GB
- **Storage**: ~694 GB

¬¥**OBS: Neste exemplo segui a sugest√£o da documenta√ß√£o das ferramentas. √â possivel alterar a quantidade de memoria e nucleos de processador conforme disponibilidade/necessidade.**¬¥

### 3.4. Conectando as VMs √†s Bridges Corretas

‚ö†Ô∏è **Aten√ß√£o cr√≠tica**: Ao criar cada VM, na aba **Network**, certifique-se de selecionar a **Bridge** correta para sua fun√ß√£o:

- **VM do pfSense**: 4 interfaces de rede
  - `net0`: vmbr0 (WAN - Internet)
  - `net1`: vmbr1 (LAN_SERVIDORES)
  - `net2`: vmbr2 (LAN_CLIENTES)
  - `net3`: vmbr3 (LAN_K8S)

- **VM do TrueNAS**:
  - `net0`: vmbr1 (LAN_SERVIDORES)

- **VMs do Cluster Kubernetes** (Load Balancers, Control Planes, Workers):
  - `net0`: vmbr3 (LAN_K8S)

### 3.5. Instalar QEMU Guest Agent nas VMs

‚ö†Ô∏è **Importante**: Ap√≥s instalar o sistema operacional em cada VM (exceto pfSense), instale o QEMU Guest Agent.

**Para Ubuntu/Debian (Load Balancers, Control Planes, Workers):**

```bash
# Ap√≥s boot da VM e instala√ß√£o do Ubuntu
sudo apt update
sudo apt install qemu-guest-agent -y

# Habilitar e iniciar servi√ßo
sudo systemctl enable qemu-guest-agent
sudo systemctl start qemu-guest-agent

# Verificar se est√° rodando
sudo systemctl status qemu-guest-agent

# Ver vers√£o
qemu-ga --version
```

**Para TrueNAS SCALE:**

O TrueNAS j√° vem com o guest agent, mas verifique se est√° habilitado:

1. Acesse TrueNAS Web UI
2. **System Settings ‚Üí Advanced**
3. Procure por "QEMU Guest Agent"
4. Certifique-se que est√° habilitado

**Benef√≠cios do Guest Agent:**
- ‚úÖ Shutdown/reboot graceful pelo Proxmox
- ‚úÖ Informa√ß√µes de IP/hostname na interface web do Proxmox
- ‚úÖ Snapshots consistentes (freeze/thaw do filesystem)
- ‚úÖ Sincroniza√ß√£o de rel√≥gio
- ‚úÖ Melhor integra√ß√£o Proxmox ‚Üî VM

---

## 4. Checklist P√≥s-Cria√ß√£o de VM

Ap√≥s criar e instalar o SO em cada VM, siga este checklist:

### 4.1. Verifica√ß√µes B√°sicas

- [ ] **Sistema operacional instalado** e funcionando
- [ ] **Hostname configurado** corretamente
  ```bash
  # Ver hostname atual
  hostnamectl
  
  # Configurar se necess√°rio
  sudo hostnamectl set-hostname k8s-control-01
  ```

- [ ] **QEMU Guest Agent** instalado e rodando (exceto pfSense)
  ```bash
  sudo systemctl status qemu-guest-agent
  ```

- [ ] **IP configurado** (est√°tico ou via DHCP)
  ```bash
  ip addr show
  ```

- [ ] **Conectividade de rede** testada
  ```bash
  # Ping para gateway (ap√≥s configurar pfSense)
  ping -c 3 192.168.3.1
  
  # Ping para internet (se j√° configurado)
  ping -c 3 8.8.8.8
  ```

- [ ] **Atualiza√ß√µes aplicadas**
  ```bash
  sudo apt update && sudo apt upgrade -y
  ```

- [ ] **Configura√ß√£o de `/etc/hosts`** (se aplic√°vel)
  ```bash
  sudo nano /etc/hosts
  # Adicionar entradas dos outros nodes
  ```

- [ ] **SSH habilitado** e funcionando
  ```bash
  sudo systemctl status ssh
  ```

- [ ] **Firewall configurado** (UFW ou iptables, se necess√°rio)
  ```bash
  # Ubuntu - habilitar UFW depois de configurar regras
  sudo ufw status
  ```

### 4.2. No Proxmox

- [ ] **Snapshot inicial criado** (estado limpo p√≥s-instala√ß√£o)
  ```bash
  # Via CLI
  qm snapshot 301 clean-install --description "Fresh Ubuntu install"
  
  # Ou via web: VM ‚Üí Snapshots ‚Üí Take Snapshot
  ```

- [ ] **Tags adicionadas** para organiza√ß√£o
  - Web UI: VM ‚Üí Options ‚Üí Tags
  - Exemplos: `k8s-control`, `k8s-worker`, `infrastructure`

- [ ] **Notes/Description** preenchido
  - Web UI: VM ‚Üí Notes
  - Adicionar informa√ß√µes √∫teis (IP, fun√ß√£o, etc.)

- [ ] **Backup configurado** (opcional mas recomendado)
  ```bash
  # Configurar via: Datacenter ‚Üí Backup
  # Ou manualmente:
  vzdump 301 --mode snapshot --compress zstd --storage local
  ```

### 4.3. Documenta√ß√£o

Mantenha um registro das VMs criadas:

```markdown
## Invent√°rio de VMs

### pfSense (VM 100)
- **IPs**: 
  - WAN: DHCP (do ISP)
  - LAN_SERVIDORES: 192.168.1.1/24
  - LAN_CLIENTES: 192.168.2.1/24
  - LAN_K8S: 192.168.3.1/24
- **Credenciais**: admin / (senha segura)
- **Fun√ß√£o**: Firewall e roteamento entre redes

### TrueNAS (VM 200)
- **IP**: 192.168.1.10 (LAN_SERVIDORES)
- **Fun√ß√£o**: Storage NFS para Kubernetes
- **Pools**: tank (2x100GB em mirror)

### Load Balancers
- **k8s-lb-01** (291): 192.168.3.11
- **k8s-lb-02** (292): 192.168.3.12
- **VIP Keepalived**: 192.168.3.10

### Control Planes
- **k8s-control-01** (301): 192.168.3.21
- **k8s-control-02** (302): 192.168.3.22
- **k8s-control-03** (303): 192.168.3.23

### Workers
- **k8s-worker-01** (311): 192.168.3.31
- **k8s-worker-02** (312): 192.168.3.32
```

---

## 5. Boas Pr√°ticas e Dicas

### 5.1. Snapshots antes de Mudan√ßas Cr√≠ticas

Sempre crie snapshots antes de:
- Atualizar kernel
- Fazer mudan√ßas de rede
- Instalar software cr√≠tico
- Alterar configura√ß√µes do Kubernetes

```bash
# Via CLI do Proxmox
qm snapshot <VMID> <snapshot-name> --description "Descri√ß√£o"

# Exemplo: Antes de atualizar kernel no control-plane-01
qm snapshot 301 pre-kernel-5.19 --description "Antes de atualizar kernel para 5.19"

# Listar snapshots
qm lis